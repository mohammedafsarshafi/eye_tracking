# -*- coding: utf-8 -*-
import cv2
import dlib
import numpy as np
import math
from collections import deque
import time

# Load face detector & landmarks predictor
detector = dlib.get_frontal_face_detector()
predictor_path = r"C:\Users\shafi\source\repos\eye_tracker\eye_tracker\models\shape_predictor_68_face_landmarks.dat"

try:
    predictor = dlib.shape_predictor(predictor_path)
    print("Shape predictor loaded!")
except RuntimeError as e:
    print(f"Error: Could not load shape predictor. Error: {e}")
    exit()

# Smoothing for angles and landmarks
ANGLE_HISTORY_SIZE = 5
LANDMARK_HISTORY_SIZE = 5
yaw_history = deque(maxlen=ANGLE_HISTORY_SIZE)
pitch_history = deque(maxlen=ANGLE_HISTORY_SIZE)
roll_history = deque(maxlen=ANGLE_HISTORY_SIZE)
landmark_history = deque(maxlen=LANDMARK_HISTORY_SIZE)

# Global variables
previous_state = "Looking at Screen"
calibrated_angles = None

def get_dynamic_model_points(landmarks):
    # Dynamically adjust 3D model points based on face size
    face_width = abs(landmarks.part(16).x - landmarks.part(0).x)
    face_height = abs(landmarks.part(8).y - landmarks.part(19).y)
    scale = face_width / 60.0  # Normalize to standard face width
    
    return np.array([
        (0.0, 0.0, 0.0),                     # Nose tip
        (0.0, -50.0 * scale, -10.0 * scale), # Chin
        (-30.0 * scale, 40.0 * scale, -10.0 * scale), # Left eye
        (30.0 * scale, 40.0 * scale, -10.0 * scale),  # Right eye
        (-25.0 * scale, -30.0 * scale, -10.0 * scale), # Left mouth corner
        (25.0 * scale, -30.0 * scale, -10.0 * scale)   # Right mouth corner
    ], dtype=np.float64)

def get_camera_matrix(frame_width):
    focal_length = frame_width  # Approximate focal length as frame width
    center = (frame_width / 2, frame_width * 3 / 8)  # Assuming 4:3 aspect ratio
    return np.array([
        [focal_length, 0, center[0]],
        [0, focal_length, center[1]],
        [0, 0, 1]
    ], dtype=np.float64), np.array([0.1, 0.1, 0.0, 0.0], dtype=np.float64).reshape(4, 1)

def get_head_pose_angles(image_points, camera_matrix, dist_coeffs):
    success, rotation_vector, translation_vector = cv2.solvePnP(
        model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
    )
    if not success:
        print("SolvePnP failed!")
        return None

    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
    sy = math.sqrt(rotation_matrix[0, 0]**2 + rotation_matrix[1, 0]**2)
    singular = sy < 1e-6

    if not singular:
        pitch = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])
        yaw = math.atan2(-rotation_matrix[2, 0], sy)
        roll = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])
    else:
        pitch = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])
        yaw = math.atan2(-rotation_matrix[2, 0], sy)
        roll = 0

    return np.degrees(pitch), np.degrees(yaw), np.degrees(roll)

def smooth_angle(angle_history, new_angle):
    angle_history.append(new_angle)
    return np.median(angle_history)  # Use median for robustness

def smooth_landmarks(new_points):
    landmark_history.append(new_points)
    return np.mean(landmark_history, axis=0)

def draw_pose_axes(frame, rotation_vector, translation_vector, camera_matrix, dist_coeffs):
    points = np.float32([[100, 0, 0], [0, 100, 0], [0, 0, 100], [0, 0, 0]]).reshape(-1, 3)
    projected_points, _ = cv2.projectPoints(points, rotation_vector, translation_vector, camera_matrix, dist_coeffs)
    projected_points = projected_points.astype(np.int32)
    
    origin = tuple(projected_points[3].ravel())
    cv2.line(frame, origin, tuple(projected_points[0].ravel()), (0, 0, 255), 3)  # X-axis (red)
    cv2.line(frame, origin, tuple(projected_points[1].ravel()), (0, 255, 0), 3)  # Y-axis (green)
    cv2.line(frame, origin, tuple(projected_points[2].ravel()), (255, 0, 0), 3)  # Z-axis (blue)

def process_head_pose(frame, calibrated_angles=None, thresholds=(5, 8, 3)):
    global previous_state, model_points

    if frame is None or frame.size == 0:
        print("Error: Frame is None or empty!")
        return None, "No Frame"

    # Frame validation
    if frame.dtype != np.uint8:
        frame = frame.astype(np.uint8)
    if len(frame.shape) == 2:
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
    elif len(frame.shape) == 3 and frame.shape[2] == 4:
        frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    camera_matrix, dist_coeffs = get_camera_matrix(frame.shape[1])

    faces = detector(gray, 1)
    head_direction = "No Face Detected"

    if not faces:
        print("No faces detected!")
        return frame, head_direction

    face = faces[0]
    landmarks = predictor(gray, face)
    
    # Dynamically adjust model points
    model_points = get_dynamic_model_points(landmarks)
    
    # Smooth landmarks
    image_points = np.array([
        (landmarks.part(30).x, landmarks.part(30).y),  # Nose tip
        (landmarks.part(8).x, landmarks.part(8).y),    # Chin
        (landmarks.part(36).x, landmarks.part(36).y),  # Left eye
        (landmarks.part(45).x, landmarks.part(45).y),  # Right eye
        (landmarks.part(48).x, landmarks.part(48).y),  # Left mouth corner
        (landmarks.part(54).x, landmarks.part(54).y)   # Right mouth corner
    ], dtype=np.float64)
    image_points = smooth_landmarks(image_points)

    angles = get_head_pose_angles(image_points, camera_matrix, dist_coeffs)
    if angles is None:
        print("Head pose angles calculation failed!")
        return frame, head_direction

    pitch, yaw, roll = angles
    pitch = smooth_angle(pitch_history, pitch)
    yaw = smooth_angle(yaw_history, yaw)
    roll = smooth_angle(roll_history, roll)
    print(f"Smoothed angles - Pitch: {pitch:.2f}, Yaw: {yaw:.2f}, Roll: {roll:.2f}")

    # Draw pose axes for debugging
    success, rotation_vector, translation_vector = cv2.solvePnP(
        model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
    )
    if success:
        draw_pose_axes(frame, rotation_vector, translation_vector, camera_matrix, dist_coeffs)

    if calibrated_angles is None:
        return frame, (pitch, yaw, roll)

    pitch_offset, yaw_offset, roll_offset = calibrated_angles
    pitch_threshold, yaw_threshold, roll_threshold = thresholds

    if (abs(yaw - yaw_offset) <= yaw_threshold and 
        abs(pitch - pitch_offset) <= pitch_threshold and 
        abs(roll - roll_offset) <= roll_threshold):
        current_state = "Looking at Screen"
    elif yaw < yaw_offset - yaw_threshold - 1:
        current_state = "Looking Left"
    elif yaw > yaw_offset + yaw_threshold + 1:
        current_state = "Looking Right"
    elif pitch > pitch_offset + pitch_threshold + 1:
        current_state = "Looking Up"
    elif pitch < pitch_offset - pitch_threshold - 1:
        current_state = "Looking Down"
    elif abs(roll - roll_offset) > roll_threshold + 1:
        current_state = "Tilted"
    else:
        current_state = previous_state

    previous_state = current_state
    head_direction = current_state
    return frame, head_direction

def main():
    print(f"OpenCV Version: {cv2.__version__}")
    print(f"dlib Version: {dlib.__version__}")

    cap = None
    max_retries = 3
    for attempt in range(max_retries):
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if cap.isOpened():
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
            break
        print(f"Attempt {attempt + 1} failed. Retrying...")
        time.sleep(1)
    if not cap.isOpened():
        print("Error: Could not open webcam!")
        return

    # Calibration
    print("Calibrating... Look straight at the screen for 7 seconds.")
    start_time = time.time()
    angle_list = []
    successful_frames = 0
    frame_count = 0
    while time.time() - start_time < 7:
        ret, frame = cap.read()
        if ret:
            result_frame, temp_angles = process_head_pose(frame)
            if isinstance(temp_angles, tuple) and len(temp_angles) == 3:
                angle_list.append(temp_angles)
                successful_frames += 1
                print(f"Calibration frame {successful_frames}: {temp_angles}")
            display_frame = frame if result_frame is None else result_frame
            cv2.imshow("Calibration", display_frame)
        frame_count += 1
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    if successful_frames >= 5 and angle_list:
        calibrated_angles = np.median(angle_list, axis=0)
        # Dynamic thresholds based on calibration variance
        angle_variance = np.std(angle_list, axis=0)
        thresholds = (
            max(4, min(6, angle_variance[0] * 2)),  # Pitch
            max(6, min(9, angle_variance[1] * 2)),  # Yaw
            max(2, min(4, angle_variance[2] * 2))   # Roll
        )
        print(f"Calibration complete: {calibrated_angles}, Thresholds: {thresholds}")
    else:
        print("Calibration failed. Using defaults.")
        calibrated_angles = (0.0, 0.0, 0.0)
        thresholds = (5, 8, 3)

    # Head pose tracking
    print("Starting head pose tracking...")
    while True:
        ret, frame = cap.read()
        if ret:
            result_frame, head_direction = process_head_pose(frame, calibrated_angles, thresholds)
            display_frame = frame if result_frame is None else result_frame
            cv2.putText(display_frame, head_direction, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.imshow("Head Pose Tracking", display_frame)
            print(f"Frame {frame_count}: Head direction: {head_direction}")
        frame_count += 1
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
